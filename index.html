<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Bits and bytes - Apurva Nandan : Observations and Experiences from Web engineering and Big data. ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Bits and bytes - Apurva Nandan</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/apurva3000/techblog">View on GitHub</a>

          <h1 id="project_title">Bits and bytes - Apurva Nandan</h1>
          <h2 id="project_tagline">Observations and Experiences from Web engineering and Big data. </h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/apurva3000/techblog/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/apurva3000/techblog/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="fixing-the-namenode-blocks-health-alert-in-hdfs" class="anchor" href="#fixing-the-namenode-blocks-health-alert-in-hdfs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fixing the NameNode Blocks Health alert in HDFS</h3>

<p>If you're using Ambari, the red alert pops up as "NameNode Blocks Health" , and as per the description from Hortonworks "This service-level alert is triggered if the number of corrupt or missing blocks exceeds the configured critical threshold."
That being said, it means that the blocks in HDFS have gone corrupt. </p>

<p>To check what files have been corrupted, execute the command (to be run with superuser privileges):
<code>sudo -u hdfs hadoop fsck -list-corruptfileblocks</code></p>

<p>To fix the corrupted blocks, try to execute the following command:
<code>sudo -u hdfs hadoop fsck / -delete</code></p>

<h3>
<a id="how-to-connect-jupyter-ipython-notebook-to-spark-running-on-standalone-or-yarn" class="anchor" href="#how-to-connect-jupyter-ipython-notebook-to-spark-running-on-standalone-or-yarn" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to connect Jupyter ipython notebook to Spark running on standalone or Yarn</h3>

<p>Coming soon.</p>

<h3>
<a id="hortonworks-nfsgateway-configuration" class="anchor" href="#hortonworks-nfsgateway-configuration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>HortonWorks NFSGateway Configuration</h3>

<p>NFSGateway allows you to browse your HDFS file system using a local file system using NFS. Not to forget, you could directly put files in HDFS by copying your files in the local file system.</p>

<p>If you happen to use NFSGateway for your Hortonworks cluster <em>(I am using HDP 2.3)</em>, you would often be puzzled about where to find the mount point location i.e. The filesystem where you can see all the files being kept in hdfs.
Although, Ambari will show you that NFSGateway is successfully running in the dashboard, it means it has only started the rpcbind and nfs services <em>(I run it on my master node which happens to be my namenode)</em></p>

<p>So, you need to mount the hdfs to a location on your local filesystem manually. And here's how you can do it:</p>

<p>First create a directory named say for example, <code>/hdfs-local</code> (requires sudo rights)</p>

<p>Then Run the following command (Requires sudo rights)</p>

<p><code>mount -t nfs -o vers=3,proto=tcp,nolock localhost:/ /hdfs-local</code></p>

<p>NOTE: This assumes you are running the NFSGateway on the current node where you're trying to mount it (localhost used in the config)</p>

<h3>
<a id="repartition-vs-coalesce" class="anchor" href="#repartition-vs-coalesce" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repartition vs Coalesce</h3>

<p>As you become familiar with Spark, you would come across a functionality called repartition. This essentially tries to reconfigure the number of partitions of your original RDD. You require them to increase or decrease the total number of partitions. For example, when the default number of partitions are quite high and thus during any operation - huge number of tasks are generated by Spark. Or, If the number of partitions are too few and in order to use the power of parallelism , you ensure that there are enough partitions of your RDD.</p>

<p>Repartitioning actually tries to move the data all over the network (shuffle mode on) to increase/decrease the number of partitions. Any shuffle operation is costly when it comes to large operations. Hence, a common suggestion is to try another functionality of Spark called 'Coalesce' - which tries to combine the number of partitions without actually performing the costly shuffle. 
The point to be noted is, this can be only used to decrease the number of partitions as compared to Repartition which can both increase or decrease it.</p>

<h3>
<a id="spark-shuffle-tuning-for-optimal-performance-with-joins-in-dataframes" class="anchor" href="#spark-shuffle-tuning-for-optimal-performance-with-joins-in-dataframes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spark Shuffle tuning for optimal performance with Joins in Dataframes</h3>

<p>It was often the case in my work that I used to perform joins with two dataframes (I used dataframes because they are much more optimized than the RDDs when it comes to shuffle operations), but since the join is essentially a cartesian join, the number of partitions would get multiplied and I would often land with huge number of tasks. </p>

<p>They key here is to tune a parameter called <code>spark.sql.shuffle.partitions</code> which has a default value of 200. I tried lowering it and then successfully reduced the number of tasks which spark used to spawn.</p>

<h3>
<a id="performing-background-tasks-in-spring-and-message-queues" class="anchor" href="#performing-background-tasks-in-spring-and-message-queues" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performing Background tasks in Spring and Message Queues</h3>

<p>Coming soon</p>

<h3>
<a id="using-nginx-to-secure-your-web-services-better" class="anchor" href="#using-nginx-to-secure-your-web-services-better" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using nginx to secure your web services better</h3>

<p>Coming soon</p>

<h3>
<a id="the-importance-of-forward-slash-in-nginx" class="anchor" href="#the-importance-of-forward-slash-in-nginx" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Importance of Forward Slash in nginx</h3>

<p>Often when using ngnix as a reverse proxy and providing the proxy pass as a URL to, for example, some service running on a webserver, there are some problems when the user tries to use the nginx url.</p>

<p>For example in this configuration</p>

<pre><code>location /resources {
    proxy_pass   http://tomcat-backend-server:8080/ResourceStatus/rest/status/;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
    proxy_set_header Host $host;
}
</code></pre>

<p>If I try to access the url by http://ngnix-proxy-server-address/resources , this would work fine, but if I or my broswer puts a forward slash http://ngnix-proxy-server-address/resources/, then this would fail as the mapping for tomcat backend server then becomes ResourceStatus/rest/status//, which is undefined. 
Hence, when providing the proxy_pass, remove the forward slash from the end of the url, to avoid any problems.</p>

<h3>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h3>

<p>Blog of <a href="https://github.com/apurva3000" class="user-mention">@apurva3000</a> Check out my repos at <a href="https://github.com/apurva3000">repo list</a></p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Bits and bytes - Apurva Nandan maintained by <a href="https://github.com/apurva3000">apurva3000</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
